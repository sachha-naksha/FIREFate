{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housekeeping Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, scanpy as sc, matplotlib.pyplot as plt, os\n",
    "from scipy.stats import hypergeom\n",
    "import celloracle as co, glob, pickle\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "import itertools, math, random\n",
    "import networkx as nx\n",
    "\n",
    "# visualization settings required to see plots in jupyter notebook\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [6, 4.5]\n",
    "plt.rcParams[\"savefig.dpi\"] = 300\n",
    "\n",
    "wd = '/ocean/projects/cis240075p/skeshari/igvf/bcell2/male_donor/'\n",
    "out_path = os.path.join(wd, 'out_data', 'lf_enrich')\n",
    "os.makedirs(f\"{out_path}/figures\", exist_ok=True)\n",
    "os.makedirs(f\"{out_path}/out_files\", exist_ok=True)\n",
    "sc.settings.figdir = f\"{out_path}/figures\"\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for building the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Functions for building networks and TF lists\n",
    "# ------------------------------------------------------------\n",
    "def create_combined_links_for_cluster_fusion(cluster_fusion, links_after_fit, quantile = 0.9):\n",
    "    # Create combined links and finding threshold for edge strength\n",
    "    combined_links = pd.DataFrame()\n",
    "    for cluster in cluster_fusion:\n",
    "        combined_links_og = links_after_fit[cluster]  # Convert to DataFrame\n",
    "        combined_links_og['cluster'] = cluster\n",
    "        combined_links = pd.concat([combined_links, combined_links_og], axis=0)\n",
    "        combined_links[combined_links['cluster']==cluster]['coef_abs'].plot(kind='hist', bins=20, alpha=0.5)\n",
    "    threshold = abs(combined_links['coef_abs']).quantile(quantile)\n",
    "    print(\"Threshold for edge strength:\", threshold)\n",
    "    combined_links['strength'] = combined_links['coef_abs'].apply(lambda x: 0 if x<threshold else 1)\n",
    "    print(\"strong edges count:\", combined_links['strength'].value_counts())\n",
    "    plt.axvline(x=threshold, color='red', linestyle='--', linewidth=1)\n",
    "    plt.title(f'Threshold for edge strength: {threshold}')\n",
    "    plt.xlabel('Absolute Coefficient Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.savefig(f\"{out_path}/figures/combined_links_cutoff_histogram_{cluster_fusion}.pdf\")\n",
    "    plt.close()\n",
    "    return combined_links, threshold\n",
    "\n",
    "def filter_combined_links_and_build_grn(combined_links,threshold):\n",
    "    # Filter combined links and create GRN\n",
    "    # combined_links['strength'] = combined_links['coef_abs'].apply(lambda x: 0 if x<threshold else 1)\n",
    "    grn = nx.MultiDiGraph()\n",
    "    for _, row in combined_links.iterrows():\n",
    "        grn.add_edge(\n",
    "            row['source'],\n",
    "            row['target'],\n",
    "            weight=row['coef_mean'],\n",
    "            cluster=row['cluster'],\n",
    "            strength=row['strength'],\n",
    "        )\n",
    "    edges_df = pd.DataFrame([\n",
    "        {\n",
    "            'source': u,\n",
    "            'target': v,\n",
    "            'key': k,\n",
    "            **d\n",
    "        }\n",
    "        for u, v, k, d in grn.edges(keys=True, data=True)\n",
    "        ])\n",
    "    return grn, edges_df\n",
    "\n",
    "def filter_network_score_data(cluster_fusion, network_scores):\n",
    "    combined_network_scores = pd.concat([network_scores[network_scores['cluster'] == int(cluster_id)] for cluster_id in cluster_fusion]) \\\n",
    "                                        .sort_values(by='degree_centrality_out', ascending=False) \\\n",
    "                                        .loc[lambda df: (~df.index.duplicated(keep='first'))]\n",
    "    return combined_network_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for doing the enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Functions for doing enrichments\n",
    "# ------------------------------------------------------------\n",
    "def enrichment(P,p,S,s):\n",
    "    # P is the total number of items in the population ===> Total starting genes in SLIDE\n",
    "    # p is the number of successes in the population ===> Downstream genes\n",
    "    # S is the sample size ===> Total SLIDE genes in LF\n",
    "    # s is the number of successes in the sample ===> Common targets in LF\n",
    "    p_value = 1 - hypergeom.cdf(s-1, P, p, S)   # Compute the p-value for observing X or more successes\n",
    "    score = math.log2((s/S)/(p/P))\n",
    "    return (score, p_value)\n",
    "\n",
    "def get_SLIDE_GRN_enrichment(edges_df,cc_dict,cluster_fusion,ord_tf,slide_features,slide_starting_genes,total_source,case):\n",
    "    possible_TF_combinations = list(itertools.combinations(total_source, ord_tf))\n",
    "    strnth_cnd_TF_comb = list(itertools.product([0, 1], repeat=ord_tf))\n",
    "    cc_dict.setdefault(cluster_fusion, {}).setdefault(ord_tf, [])\n",
    "    for TF_comb in tqdm(possible_TF_combinations):\n",
    "        # print(f\"TF_comb: {TF_comb}\")\n",
    "        # FILTER1: Finding common targets from GRN for the TF_comb\n",
    "        edges_grouped = edges_df[edges_df['source'].isin(TF_comb)].groupby(['source']).agg(list)\n",
    "        if edges_grouped.empty:\n",
    "            print(f\"No edges found for TF_comb: {TF_comb}\")\n",
    "            continue\n",
    "        common_targets_from_grn = set.intersection(*map(set, edges_grouped.target.values))\n",
    "        for condition in strnth_cnd_TF_comb:\n",
    "            if len(common_targets_from_grn) == 0:\n",
    "                cc_dict[cluster_fusion][ord_tf].append([TF_comb, (condition), (0, 1), ([],[]), case, (pd.DataFrame(),pd.DataFrame())])\n",
    "            else:\n",
    "                # FILTER2: Vectorized filtering on (TF, condition) and common targets\n",
    "                filter_df = pd.DataFrame({'source': TF_comb, 'strength': condition})\n",
    "                filtered = edges_df.merge(filter_df, on=['source', 'strength'])\n",
    "                filtered = filtered[filtered['target'].isin(common_targets_from_grn)]\n",
    "\n",
    "                # Keep only targets regulated by all (TF, condition) pairs\n",
    "                common_targets = (filtered.groupby('target')[['source', 'strength']].nunique().eq(len(filter_df)).all(axis=1))\n",
    "                filtered_edges_df = filtered[filtered['target'].isin(common_targets[common_targets].index)]\n",
    "                if filtered_edges_df.empty:\n",
    "                    cc_dict[cluster_fusion][ord_tf].append([TF_comb, (condition), (0, 1), ([], []), case, (filtered_edges_df, pd.DataFrame())])\n",
    "                else:\n",
    "                    # Drop duplicates based on source and target, keeping the one with the maximum absolute weight\n",
    "                    filtered_edges_df_unique = filtered_edges_df.loc[filtered_edges_df.groupby(['source', 'target'])['weight'].apply(lambda x: x.abs().idxmax())]\n",
    "                    dwn_list = list(filtered_edges_df_unique['target'].unique())\n",
    "                    dwngene = len(dwn_list)\n",
    "                    # FILTER3: Finding intersection with latent features\n",
    "                    cmn_list = list(set(filtered_edges_df_unique['target']).intersection(slide_features))\n",
    "                    common = len(cmn_list)\n",
    "                    if common == 0:\n",
    "                        cc_dict[cluster_fusion][ord_tf].append([TF_comb, (condition), (0, 1), ([], [dwn_list]), case, (filtered_edges_df, filtered_edges_df_unique)])\n",
    "                    else:\n",
    "                        enrich = enrichment(slide_starting_genes, dwngene, len(slide_features), common)\n",
    "                        cc_dict[cluster_fusion][ord_tf].append([TF_comb, (condition), enrich, (cmn_list, dwn_list), case, (filtered_edges_df,filtered_edges_df_unique)])\n",
    "    return cc_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Functions for optionally generating the dataframe to be saved\n",
    "# ------------------------------------------------------------\n",
    "def _create_enrich_df(cc_dict, cluster_fusion, order_of_combination, filter):\n",
    "    enrichment_df = pd.DataFrame(cc_dict[cluster_fusion][order_of_combination], columns=['TF', 'condition', 'ES', 'Genes', 'case', 'dfs'])\n",
    "    enrichment_df[['score', 'p_value']] = pd.DataFrame(enrichment_df['ES'].tolist(), index=enrichment_df.index)\n",
    "    enrichment_df[['common' , 'dwnstrm']] = pd.DataFrame(enrichment_df['Genes'].tolist(), index=enrichment_df.index)\n",
    "    enrichment_df = enrichment_df.drop(columns=['ES', 'Genes'])\n",
    "    filter_enrichment_df = (enrichment_df['condition'].isin(filter))\n",
    "    enrichment_df = enrichment_df[filter_enrichment_df].sort_values(by='score', ascending=False)\n",
    "    if order_of_combination == 1:\n",
    "        non_zero_score_df = (enrichment_df['p_value']<0.05) \\\n",
    "                                & (enrichment_df['dwnstrm'].apply(len)>2) \\\n",
    "                                & (enrichment_df['common'].apply(len)>1) \\\n",
    "                                & (enrichment_df['score']>0) \\\n",
    "                                & (enrichment_df['case'].isin(['slide', 'net', 'rnd']))\n",
    "        enrichment_df = enrichment_df[non_zero_score_df].reset_index(drop=True)\n",
    "    elif order_of_combination == 2:\n",
    "        non_zero_score_df = (enrichment_df['p_value']<0.05) \\\n",
    "                                & (enrichment_df['dwnstrm'].apply(len)>2) \\\n",
    "                                & (enrichment_df['common'].apply(len)>1) \\\n",
    "                                & (enrichment_df['score']>0) \\\n",
    "                                & (enrichment_df['case'].isin(['slide', 'net', 'rnd']))\n",
    "        # Keep the zero scores for the cliffs delta analysis\n",
    "        enrichment_df.loc[~non_zero_score_df, 'score'] = 0\n",
    "        enrichment_df.loc[~non_zero_score_df, 'p_value'] = 1.0\n",
    "\n",
    "    return enrichment_df\n",
    "\n",
    "def create_enrichment_df(cc_dict, cluster_fusion, order_of_combination, filter = None, suffix=None, write = True):\n",
    "    if order_of_combination ==1 and filter is None:\n",
    "        filter = [(1,)]\n",
    "    elif order_of_combination ==2 and filter is None:\n",
    "        filter = [(1,1), (0,1), (1,0)]\n",
    "    else:\n",
    "        raise ValueError(\"Currently only supports order_of_combination= 1 or 2\")\n",
    "    enrichment_df = _create_enrich_df(cc_dict, cluster_fusion, order_of_combination, filter)\n",
    "    if write:\n",
    "        enrichment_df['dfs'] = enrichment_df['dfs'].apply(lambda x: x.to_json() if isinstance(x, pd.DataFrame) else str(x))\n",
    "        enrichment_df.to_csv(f\"{out_path}/out_files/SLIDE_LF_enrichment/enriched_df_{order_of_combination}_TFs_{cluster_fusion}{suffix}.csv\", index=False)\n",
    "    return enrichment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Functions for reading in the data\n",
    "# ------------------------------------------------------------\n",
    "def fetch_GRN_data(GRN_wd):\n",
    "    # Read oracle links after fitting\n",
    "    oracle = co.load_hdf5(f\"{GRN_wd}/out_data/grn_inference/out_files/oracle_fitted.celloracle.oracle\")\n",
    "    GRN_network_scores = pd.read_csv(f\"{GRN_wd}/out_data/grn_inference/out_files/ridge_fitted_2_merged_network_scores.csv\", index_col=0)\n",
    "    GRN_TFs = oracle.all_regulatory_genes_in_TFdict\n",
    "    GRN_links_after_fit = {key: [] for key in oracle.coef_matrix_per_cluster.keys()}\n",
    "    for cluster in oracle.coef_matrix_per_cluster.keys():\n",
    "        cluster_specific_links = oracle.coef_matrix_per_cluster[cluster].stack().reset_index()\n",
    "        cluster_specific_links.columns = ['source', 'target', 'coef_mean']\n",
    "        cluster_specific_links = cluster_specific_links[cluster_specific_links ['coef_mean'] != 0].reset_index(drop=True)\n",
    "        cluster_specific_links['coef_abs'] = np.abs(cluster_specific_links['coef_mean'])\n",
    "        GRN_links_after_fit[cluster] = cluster_specific_links\n",
    "        # links_after_fit[cluster]['weight'] = links_after_fit[cluster]['coef_abs'] * links_after_fit[cluster]['-logp']\n",
    "        # GRN_TFs = GRN_TFs + list(links_after_fit[cluster].source.unique())\n",
    "    return GRN_links_after_fit, GRN_network_scores, GRN_TFs\n",
    "\n",
    "def read_slide_data(experiment, wd):\n",
    "    # Read SLIDE data for the experiment\n",
    "    feature_files = glob.glob(f\"{wd}/out_data/out_other_methods/SLIDE_Runs/{experiment}/*/*feature_list*\")\n",
    "    if experiment == 'IRF4_KO':\n",
    "        feature_data = [pd.read_csv(file, sep='\\t', header=0) for file in feature_files if 'Z4' in file or 'Z36' in file]\n",
    "    else:\n",
    "        feature_data = [pd.read_csv(file, sep='\\t', header = 0) for file in feature_files]\n",
    "    feature_data = pd.concat(feature_data)\n",
    "    slide_features = set(feature_data['names'])\n",
    "    return slide_features\n",
    "\n",
    "#### Create input dictionary for the experiments\n",
    "input_dict = {\n",
    "    'experiment': ['PRDM1_KO', 'IRF4_KO', 'GC_PB', 'PB_ABC', 'GC_ABC'],\n",
    "    'slide_starting_genes': [4472, 4500, 4725, 3420, 4603],\n",
    "    'clusters_of_interest': [['3','7'], ['3','7'], ['7','3'], ['1','7'], ['1','3']],\n",
    "    'order_fr_clust': [[2], [2], [2], [2], [2]],\n",
    "    'order_fr_tfcomb': [[1], [1], [2], [2], [2]],\n",
    "    'weight': ['strength', 'strength', 'strength', 'strength', 'strength'],\n",
    "}\n",
    "input_df = pd.DataFrame(input_dict)\n",
    "\n",
    "#### Assign the input parameters\n",
    "i=1\n",
    "experiment = input_df['experiment'][i]\n",
    "slide_starting_genes = input_df['slide_starting_genes'][i]\n",
    "clusters_of_interest = input_df['clusters_of_interest'][i]\n",
    "order_fr_clust = input_df['order_fr_clust'][i]\n",
    "order_fr_tfcomb = input_df['order_fr_tfcomb'][i]\n",
    "weight = input_df['weight'][i]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Reading the data\n",
    "# ------------------------------------------------------------\n",
    "#Read the GRN data and slide features\n",
    "GRN_wd = '/ocean/projects/cis240075p/skeshari/igvf/bcell1/male_donor'\n",
    "GRN_links_after_fit, GRN_network_scores, GRN_TFs = fetch_GRN_data(GRN_wd)\n",
    "slide_features = read_slide_data(experiment, wd)\n",
    "cluster_fusions = []\n",
    "for ord_clus in order_fr_clust:\n",
    "    cluster_fusions += list(itertools.combinations(clusters_of_interest, ord_clus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold for edge strength: 0.20740218693294243\n",
      "strong edges count: 0    18000\n",
      "1     2000\n",
      "Name: strength, dtype: int64\n",
      "Running enrichment for IRF4_KO, ('3', '7'), 1 TFs, case = slide\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [00:02<00:00, 30.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating enrichment dataframe for IRF4_KO, ('3', '7'), 1 TFs, all cases\n",
      "slide    2\n",
      "Name: case, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_df_pickle = True\n",
    "for cluster_fusion in cluster_fusions:\n",
    "    combined_links, threshold = create_combined_links_for_cluster_fusion(cluster_fusion, GRN_links_after_fit, quantile=0.90)\n",
    "    grn, edges_df = filter_combined_links_and_build_grn(combined_links, threshold)\n",
    "\n",
    "    fig = edges_df.groupby(['strength', 'key']).size().unstack().plot(kind='bar', stacked=True)\n",
    "    plt.xlabel('Strength')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Key and Strength')\n",
    "    plt.savefig(f\"{out_path}/figures/combined_links_key_strength_{cluster_fusion}{experiment}.pdf\")\n",
    "    plt.close()\n",
    "    # ------------------------------------------------------------\n",
    "    # Creating the TF lists\n",
    "    # ------------------------------------------------------------\n",
    "    # Step1: Filter SLIDE data wrt GRN, since SLIDE and CO gene sets are not same\n",
    "    slide_features = slide_features.intersection(set(grn.nodes)) \n",
    "    slide_features_neighbors = []\n",
    "    for gene in slide_features:\n",
    "        slide_features_neighbors += list(grn.predecessors(gene))\n",
    "    slide_tot_TF = (slide_features.union(set(slide_features_neighbors))).intersection(GRN_TFs)\n",
    "\n",
    "    # # Step2: Creating the list of network and random TFs only for analysis for order of TF combinations >1\n",
    "    # # Read network scores\n",
    "    # combined_network_scores = filter_network_score_data(cluster_fusion, GRN_network_scores)\n",
    "    # combined_network_scores = combined_network_scores[combined_network_scores.index.isin(list(grn.nodes))] # Since SLIDE and CO gene sets are not same\n",
    "    # combined_network_scores = combined_network_scores[combined_network_scores.index.isin(GRN_TFs)] # Since I want to pick size matched set of TFs only\n",
    "    # #### Choosing size matched set of TFs from network and random\n",
    "    # net_match_TF = set(combined_network_scores.index[:len(slide_tot_TF)])\n",
    "    # # net_rnd_TF = set(random.sample(set(GRN_TFs).intersection(set(grn.nodes)), len(slide_tot_TF)))\n",
    "    # net_rnd_TF = set(random.sample(set(GRN_TFs), len(slide_tot_TF)))\n",
    "\n",
    "    # # write the TFs to file as separate csvs\n",
    "    # pd.DataFrame(slide_tot_TF).to_csv(f\"{out_path}/out_files/SLIDE_LF_{cluster_fusion}_{experiment}.csv\", index=False)\n",
    "    # pd.DataFrame(net_match_TF).to_csv(f\"{out_path}/out_files/Net_match_{cluster_fusion}_{experiment}.csv\", index=False)\n",
    "    # pd.DataFrame(net_rnd_TF).to_csv(f\"{out_path}/out_files/Net_rnd_{cluster_fusion}_{experiment}.csv\", index=False)\n",
    "    # # Create a Venn diagram for the three sets\n",
    "    # from matplotlib_venn import venn3\n",
    "    # venn = venn3([slide_tot_TF, set(net_match_TF), set(net_rnd_TF)], ('Slide', 'Net', 'Random'))\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Finally doing enrichments\n",
    "    # ------------------------------------------------------------\n",
    "    for ord_tf in order_fr_tfcomb:\n",
    "        cc_dict = {} # Initialize the dictionary to store enrichment results for each cluster fusion and order of TF combinations\n",
    "        # Step1: Deciding TF combinations for which the enrichment needs to be done\n",
    "        if ord_tf == 1:\n",
    "            cases = [(slide_tot_TF, 'slide')]\n",
    "        elif ord_tf == 2:\n",
    "            # common_TFs = slide_tot_TF.intersection(net_match_TF.intersection(net_rnd_TF)) \n",
    "            # slide_tot_TF = slide_tot_TF- common_TFs\n",
    "            # net_match_TF = net_match_TF - common_TFs\n",
    "            # net_rnd_TF = net_rnd_TF - common_TFs\n",
    "            cases = [(slide_tot_TF, 'slide')] #[(slide_tot_TF, 'slide'),(net_match_TF, 'net'),(net_rnd_TF, 'rnd')]\n",
    "        # Step2: Performing enrichment analysis\n",
    "        for TFs, case in cases:\n",
    "            print(f\"Running enrichment for {experiment}, {cluster_fusion}, {ord_tf} TFs, case = {case}\")\n",
    "            cc_dict = get_SLIDE_GRN_enrichment(edges_df,cc_dict,cluster_fusion,ord_tf,slide_features,slide_starting_genes,TFs,case)\n",
    "        # ------------------------------------------------------------\n",
    "        # Optionally generating the dataframe to be saved\n",
    "        # ------------------------------------------------------------\n",
    "        if create_df_pickle == True:\n",
    "            suffix = f\"_{experiment}\"\n",
    "            print(f\"Creating enrichment dataframe for {experiment}, {cluster_fusion}, {ord_tf} TFs, all cases\")\n",
    "            enrichment_df= create_enrichment_df(cc_dict, cluster_fusion, ord_tf, filter = None, suffix=suffix)\n",
    "            print(enrichment_df['case'].value_counts())\n",
    "            # dumping the dictionary to pickle file\n",
    "            with open(f\"{out_path}/out_files/SLIDE_LF_enrichment/cc_dict_{ord_tf}_TFs_{cluster_fusion}{suffix}.pickle\", 'wb') as f:\n",
    "                pickle.dump(cc_dict, f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "celloracle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
